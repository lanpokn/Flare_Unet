#!/usr/bin/env python3
"""
DSEC Dataset Generator - ä»é•¿ç‚«å…‰æ–‡ä»¶ä¸­æ™ºèƒ½æå–100msæ®µå¹¶å¤„ç†

åŸºäºLinuså“²å­¦ï¼š
- æ•°æ®ç»“æ„æ­£ç¡®: é¡ºåºå¤„ç† â†’ å®‰å…¨è¯»å– â†’ 100msæå– â†’ å¤šæ–¹æ³•å¤„ç† â†’ ç»Ÿä¸€å¯è§†åŒ–
- æ¶ˆé™¤ç‰¹æ®Šæƒ…å†µ: ç»Ÿä¸€å¤„ç†æµç¨‹ï¼Œå¤ç”¨ç°æœ‰å·¥å…·
- å®ç”¨ä¸»ä¹‰: å†…å­˜å®‰å…¨ï¼Œé¿å…æº¢å‡ºï¼Œæ–­ç‚¹ç»­å­˜

åŠŸèƒ½ï¼š
1. ä»flare_eventsæ–‡ä»¶å¤¹æŒ‰é¡ºåºè¯»å–é•¿H5æ–‡ä»¶
2. æ¯ä¸ªæ–‡ä»¶å†…æŒ‰æ—¶é—´é¡ºåºé‡‡æ ·ï¼ˆé—´éš”400msï¼‰ï¼š0-100ms, 400-500ms, 800-900ms, ...
3. æ™ºèƒ½è¯»å–ï¼šå…ˆè¯»æ—¶é—´æˆ³ï¼Œå†åªè¯»å–éœ€è¦çš„100msèŒƒå›´ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
4. æ–­ç‚¹ç»­å­˜ï¼šè§£æå·²æœ‰æ–‡ä»¶åï¼Œè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„æ®µ
5. è¿è¡Œæ‰€æœ‰å¤„ç†æ–¹æ³•ï¼šUNet3D, PFD, Baseline, EFR
6. ç”Ÿæˆå¯è§†åŒ–åˆ°DSEC_data/visualize

Usage:
    python src/tools/generate_dsec_dataset.py  # é¡ºåºå¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œè‡ªåŠ¨æ–­ç‚¹ç»­å­˜
    python src/tools/generate_dsec_dataset.py --debug
"""

import os
import sys
import random
import h5py
import hdf5plugin  # å¿…é¡»importä»¥æ”¯æŒgzipå‹ç¼©çš„H5æ–‡ä»¶
import numpy as np
from pathlib import Path
from datetime import datetime
import subprocess  # ä»…UNet3D inferenceéœ€è¦
import argparse
from typing import Tuple, Optional, List, Dict, Set

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.tools.event_video_generator import EventVideoGenerator
from src.data_processing.encode import load_h5_events, events_to_voxel
from src.data_processing.decode import voxel_to_events

# å¯¼å…¥å¤„ç†å™¨ç±»
sys.path.append(str(PROJECT_ROOT / 'ext' / 'PFD'))
sys.path.append(str(PROJECT_ROOT / 'ext' / 'EFR-main'))
from batch_pfd_processor import BatchPFDProcessor
from batch_efr_processor import BatchEFRProcessor


class DSECDatasetGenerator:
    """DSECæ•°æ®é›†ç”Ÿæˆå™¨ - å†…å­˜å®‰å…¨çš„100msæ®µæå–ä¸å¤„ç†"""

    def __init__(self,
                 flare_dir: str = "/mnt/e/2025/event_flick_flare/main/data/flare_events",
                 output_base: str = "DSEC_data",
                 debug: bool = False):
        """
        Args:
            flare_dir: é•¿ç‚«å…‰æ–‡ä»¶ç›®å½•ï¼ˆWSLæ ¼å¼ï¼‰
            output_base: DSEC_dataåŸºç¡€ç›®å½•
            debug: æ˜¯å¦å¯ç”¨debugæ¨¡å¼
        """
        self.flare_dir = Path(flare_dir)
        self.output_base = Path(output_base)
        self.debug = debug

        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.input_dir = self.output_base / "input"
        self.inputpfds_dir = self.output_base / "inputpfds"
        self.output_dir = self.output_base / "output"  # æ ‡å‡†æƒé‡
        self.output_full_dir = self.output_base / "output_full"
        self.output_simple_dir = self.output_base / "output_simple"
        self.output_simple_timeRandom_dir = self.output_base / "output_simple_timeRandom"
        self.output_physics_noRandom_dir = self.output_base / "output_physics_noRandom"
        self.outputbaseline_dir = self.output_base / "outputbaseline"
        self.inputefr_dir = self.output_base / "inputefr"
        self.visualize_dir = self.output_base / "visualize"

        # UNet checkpointé…ç½®ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„é¿å…WSL I/Oé—®é¢˜ï¼‰
        checkpoint_base = PROJECT_ROOT / "checkpoints"
        self.unet_checkpoints = {
            'standard': str(checkpoint_base / 'event_voxel_deflare' / 'checkpoint_epoch_0027_iter_077500.pth'),
            'full': str(checkpoint_base / 'event_voxel_deflare_full' / 'checkpoint_epoch_0032_iter_076250.pth'),  # æœ€æ–°è®­ç»ƒæƒé‡
            'simple': str(checkpoint_base / 'event_voxel_deflare_simple' / 'best_checkpoint.pth'),
            'simple_timeRandom': str(checkpoint_base / 'event_voxel_deflare_simple_timeRandom_method' / 'best_checkpoint.pth'),
            'physics_noRandom': str(checkpoint_base / 'physics_noRandom_method' / 'best_checkpoint.pth')
        }

        # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
        for dir_path in [self.input_dir, self.inputpfds_dir, self.output_dir,
                         self.output_full_dir, self.output_simple_dir,
                         self.output_simple_timeRandom_dir, self.output_physics_noRandom_dir,
                         self.outputbaseline_dir, self.inputefr_dir, self.visualize_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # è§†é¢‘ç”Ÿæˆå™¨
        self.video_generator = EventVideoGenerator(
            sensor_size=(480, 640),
            frame_duration_ms=2.5,
            fps=10
        )

        # åˆå§‹åŒ–å¤„ç†å™¨
        self.pfd_processor = BatchPFDProcessor(debug=False)
        self.efr_processor = BatchEFRProcessor(debug=False)

        print(f"ğŸš€ DSEC Dataset Generator initialized")
        print(f"ğŸ“‚ Flare source: {self.flare_dir}")
        print(f"ğŸ“‚ Output base: {self.output_base}")

    def get_sorted_flare_files(self) -> List[Path]:
        """è·å–æ’åºåçš„ç‚«å…‰æ–‡ä»¶åˆ—è¡¨ï¼ˆé¡ºåºå¤„ç†ï¼‰"""
        flare_files = sorted(list(self.flare_dir.glob("*.h5")))
        if not flare_files:
            raise FileNotFoundError(f"No H5 files found in {self.flare_dir}")

        print(f"ğŸ“„ Found {len(flare_files)} flare files (sorted)")
        return flare_files

    def get_time_range_safe(self, file_path: Path) -> Tuple[int, int]:
        """å®‰å…¨è·å–H5æ–‡ä»¶çš„æ—¶é—´èŒƒå›´ï¼ˆä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰"""
        with h5py.File(file_path, 'r') as f:
            t_data = f['events']['t']
            # åªè¯»å–é¦–å°¾å…ƒç´ æ¥ç¡®å®šæ—¶é—´èŒƒå›´
            t_min = int(t_data[0])
            t_max = int(t_data[-1])

        print(f"  Time range: {t_min/1000:.1f}ms - {t_max/1000:.1f}ms (duration: {(t_max-t_min)/1000:.1f}ms)")
        return t_min, t_max

    def generate_time_samples(self, t_min: int, t_max: int) -> List[int]:
        """
        ç”Ÿæˆæ—¶é—´é‡‡æ ·ç‚¹åˆ—è¡¨ï¼ˆé—´éš”400msï¼‰

        é‡‡æ ·ç­–ç•¥ï¼š0-100ms, 400-500ms, 800-900ms, 1200-1300ms, ...

        Args:
            t_min: æ–‡ä»¶èµ·å§‹æ—¶é—´ï¼ˆå¾®ç§’ï¼‰
            t_max: æ–‡ä»¶ç»“æŸæ—¶é—´ï¼ˆå¾®ç§’ï¼‰

        Returns:
            é‡‡æ ·èµ·å§‹æ—¶é—´åˆ—è¡¨ï¼ˆå¾®ç§’ï¼‰
        """
        samples = []
        segment_duration = 100000  # 100ms = 100,000Î¼s
        interval = 400000  # 400ms = 400,000Î¼s é—´éš”

        current_start = t_min
        while current_start + segment_duration <= t_max:
            samples.append(current_start)
            current_start += interval

        print(f"  Generated {len(samples)} time samples (400ms interval)")
        return samples

    def parse_existing_progress(self) -> Dict[str, Set[int]]:
        """
        è§£æDSEC_data/inputä¸­å·²æœ‰æ–‡ä»¶ï¼Œæ¨æ–­å¤„ç†è¿›åº¦ï¼ˆæ–­ç‚¹ç»­å­˜ï¼‰

        æ–‡ä»¶åæ ¼å¼: real_flare_{source}_t{time}ms_{datetime}.h5
        æå–ä¿¡æ¯: source_name, start_time_us

        Returns:
            {source_name: {start_time_us1, start_time_us2, ...}}
        """
        progress = {}

        for h5_file in self.input_dir.glob("real_flare_*.h5"):
            try:
                # è§£ææ–‡ä»¶å
                # ä¾‹å¦‚: real_flare_zurich_city_03_a_t34867ms_20251011_120721.h5
                stem = h5_file.stem

                # æå–source_nameå’Œtime
                parts = stem.split('_t')
                if len(parts) >= 2:
                    source_name = parts[0].replace('real_flare_', '')
                    time_part = parts[1].split('ms_')[0]
                    start_time_ms = int(time_part)
                    start_time_us = start_time_ms * 1000

                    if source_name not in progress:
                        progress[source_name] = set()
                    progress[source_name].add(start_time_us)
            except Exception as e:
                print(f"  âš ï¸  Warning: Failed to parse {h5_file.name}: {e}")
                continue

        # æ‰“å°å·²æœ‰è¿›åº¦
        if progress:
            print(f"ğŸ“Š Existing progress (æ–­ç‚¹ç»­å­˜):")
            for source, times in sorted(progress.items()):
                print(f"  {source}: {len(times)} segments processed")
        else:
            print(f"ğŸ“Š No existing progress found, starting from scratch")

        return progress

    def extract_100ms_segment_safe(self, file_path: Path, start_time_us: int) -> np.ndarray:
        """
        å†…å­˜å®‰å…¨åœ°æå–100msäº‹ä»¶æ®µ

        æ ¸å¿ƒç­–ç•¥ï¼šåˆ†å—äºŒåˆ†æŸ¥æ‰¾è¾¹ç•Œç´¢å¼•ï¼Œé¿å…åŠ è½½æ•´ä¸ªæ—¶é—´æˆ³æ•°ç»„
        """
        segment_duration_us = 100000  # 100ms
        end_time_us = start_time_us + segment_duration_us

        with h5py.File(file_path, 'r') as f:
            events_group = f['events']
            t_dataset = events_group['t']
            total_events = len(t_dataset)

            # Step 1: åˆ†å—äºŒåˆ†æŸ¥æ‰¾èµ·å§‹ç´¢å¼• (é¿å…åŠ è½½å…¨éƒ¨æ•°æ®)
            chunk_size = 100000  # æ¯æ¬¡è¯»å–10ä¸‡ä¸ªæ—¶é—´æˆ³
            idx_start = self._binary_search_time_index(
                t_dataset, start_time_us, 0, total_events, chunk_size, find_start=True
            )

            # Step 2: ä»èµ·å§‹ç´¢å¼•é™„è¿‘æŸ¥æ‰¾ç»“æŸç´¢å¼•
            idx_end = self._binary_search_time_index(
                t_dataset, end_time_us, idx_start, total_events, chunk_size, find_start=False
            )

            if idx_start >= idx_end:
                print(f"  âš ï¸  No events in selected time window")
                return np.empty((0, 4))

            # Step 3: åªè¯»å–æ‰¾åˆ°çš„èŒƒå›´ï¼ˆå†…å­˜å®‰å…¨ï¼‰
            t = t_dataset[idx_start:idx_end]
            x = events_group['x'][idx_start:idx_end]
            y = events_group['y'][idx_start:idx_end]
            p = events_group['p'][idx_start:idx_end]

            # ææ€§è½¬æ¢ï¼ˆç»Ÿä¸€ä¸º-1/1æ ¼å¼ï¼‰
            p_converted = np.where(p == 1, 1, -1)

            # ç»„åˆæˆ(N,4)æ ¼å¼
            events_segment = np.column_stack((t, x, y, p_converted))

        print(f"  âœ… Extracted {len(events_segment):,} events from segment")
        return events_segment

    def _binary_search_time_index(self, t_dataset, target_time: int,
                                   left: int, right: int, chunk_size: int,
                                   find_start: bool = True) -> int:
        """
        åˆ†å—äºŒåˆ†æŸ¥æ‰¾æ—¶é—´ç´¢å¼•ï¼ˆå†…å­˜å‹å¥½ï¼‰

        Args:
            t_dataset: H5 datasetå¯¹è±¡ï¼ˆä¸åŠ è½½åˆ°å†…å­˜ï¼‰
            target_time: ç›®æ ‡æ—¶é—´æˆ³ï¼ˆå¾®ç§’ï¼‰
            left, right: æœç´¢èŒƒå›´
            chunk_size: æ¯æ¬¡è¯»å–çš„äº‹ä»¶æ•°é‡
            find_start: True=æŸ¥æ‰¾>=targetçš„ç¬¬ä¸€ä¸ªç´¢å¼•, False=æŸ¥æ‰¾<targetçš„æœ€åä¸€ä¸ªç´¢å¼•+1

        Returns:
            ç´¢å¼•ä½ç½®
        """
        while left < right:
            mid = (left + right) // 2

            # åˆ†å—è¯»å–ï¼šåªè¯»å–midé™„è¿‘çš„chunk
            chunk_start = max(0, mid - chunk_size // 2)
            chunk_end = min(len(t_dataset), chunk_start + chunk_size)
            t_chunk = t_dataset[chunk_start:chunk_end]

            # åœ¨chunkå†…æ‰¾åˆ°midå¯¹åº”çš„æ—¶é—´æˆ³
            mid_offset = mid - chunk_start
            if mid_offset < 0 or mid_offset >= len(t_chunk):
                # è¾¹ç•Œæƒ…å†µï¼šç›´æ¥è¯»å–midä½ç½®
                t_mid = t_dataset[mid]
            else:
                t_mid = t_chunk[mid_offset]

            if find_start:
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª >= target_time çš„ä½ç½®
                if t_mid < target_time:
                    left = mid + 1
                else:
                    right = mid
            else:
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª >= target_time çš„ä½ç½®ï¼ˆä½œä¸ºendï¼‰
                if t_mid < target_time:
                    left = mid + 1
                else:
                    right = mid

        return left

    def save_h5_events(self, events: np.ndarray, output_path: Path):
        """ä¿å­˜äº‹ä»¶åˆ°H5æ–‡ä»¶ï¼ˆæ ‡å‡†DSECæ ¼å¼ï¼‰"""
        with h5py.File(output_path, 'w') as f:
            events_group = f.create_group('events')
            events_group.create_dataset('t', data=events[:, 0].astype(np.int64),
                                       compression='gzip', compression_opts=9)
            events_group.create_dataset('x', data=events[:, 1].astype(np.uint16),
                                       compression='gzip', compression_opts=9)
            events_group.create_dataset('y', data=events[:, 2].astype(np.uint16),
                                       compression='gzip', compression_opts=9)
            events_group.create_dataset('p', data=events[:, 3].astype(np.int8),
                                       compression='gzip', compression_opts=9)

    def generate_filename(self, source_file: Path, start_time_us: int) -> str:
        """
        ç”ŸæˆDSECæ ‡å‡†æ–‡ä»¶å

        æ ¼å¼: real_flare_{source}_t{time}ms_{datetime}.h5
        """
        source_name = source_file.stem  # ä¾‹å¦‚ï¼šzurich_city_03_a
        time_ms = int(start_time_us / 1000)
        datetime_str = datetime.now().strftime("%Y%m%d_%H%M%S")

        filename = f"real_flare_{source_name}_t{time_ms}ms_{datetime_str}.h5"
        return filename

    def run_unet_inference(self, input_h5: Path, output_h5: Path, checkpoint_path: str, variant_name: str = "standard"):
        """
        è¿è¡ŒUNet3Dæ¨ç†

        Args:
            input_h5: è¾“å…¥H5æ–‡ä»¶
            output_h5: è¾“å‡ºH5æ–‡ä»¶
            checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
            variant_name: æƒé‡å˜ä½“åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        """
        # ä¸´æ—¶ä¿®æ”¹inference_config.yamlä¸­çš„checkpointè·¯å¾„
        import yaml
        config_path = PROJECT_ROOT / "configs" / "inference_config.yaml"

        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)

        original_path = config['model']['path']
        config['model']['path'] = checkpoint_path

        # å†™å…¥ä¸´æ—¶é…ç½®
        temp_config_path = PROJECT_ROOT / f"configs/temp_inference_{variant_name}.yaml"
        with open(temp_config_path, 'w') as f:
            yaml.dump(config, f)

        cmd = [
            sys.executable, "main.py", "inference",
            "--config", str(temp_config_path),
            "--input", str(input_h5),
            "--output", str(output_h5)
        ]

        print(f"    ğŸ”§ Running UNet3D ({variant_name})...")
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=PROJECT_ROOT, timeout=300)

            if result.returncode == 0:
                print(f"    âœ… UNet3D ({variant_name}) completed")
                success = True
            else:
                print(f"    âŒ UNet3D ({variant_name}) failed")
                if result.stderr:
                    # åªæ‰“å°å…³é”®é”™è¯¯ä¿¡æ¯
                    error_lines = result.stderr.strip().split('\n')
                    for line in error_lines[-5:]:  # åªæ‰“å°æœ€å5è¡Œ
                        if 'ERROR' in line or 'Error' in line:
                            print(f"       {line}")
                success = False
        except subprocess.TimeoutExpired:
            print(f"    âŒ UNet3D ({variant_name}) timeout (>5min)")
            success = False
        except Exception as e:
            print(f"    âŒ UNet3D ({variant_name}) exception: {e}")
            success = False
        finally:
            # æ¸…ç†ä¸´æ—¶é…ç½®
            if temp_config_path.exists():
                temp_config_path.unlink()

        return success

    def run_all_unet_variants(self, input_h5: Path, filename: str) -> dict:
        """
        è¿è¡Œæ‰€æœ‰UNetæƒé‡å˜ä½“ï¼ˆé¡ºåºæ‰§è¡Œï¼Œå¸¦é”™è¯¯å¤„ç†ï¼‰

        Returns:
            {variant_name: output_h5_path} (åªåŒ…å«æˆåŠŸçš„)
        """
        outputs = {}
        variants = [
            ('standard', self.output_dir),
            ('full', self.output_full_dir),
            ('simple', self.output_simple_dir),
            ('simple_timeRandom', self.output_simple_timeRandom_dir),
            ('physics_noRandom', self.output_physics_noRandom_dir)
        ]

        for variant_name, output_dir in variants:
            output_h5 = output_dir / filename
            checkpoint_path = self.unet_checkpoints[variant_name]

            # éªŒè¯checkpointå­˜åœ¨
            if not Path(checkpoint_path).exists():
                print(f"    âš ï¸  UNet3D ({variant_name}) skipped - checkpoint not found")
                continue

            # è¿è¡Œæ¨ç†
            success = self.run_unet_inference(input_h5, output_h5, checkpoint_path, variant_name)

            # åªè®°å½•æˆåŠŸçš„è¾“å‡º
            if success and output_h5.exists():
                outputs[variant_name] = output_h5

        print(f"    ğŸ“Š UNet variants completed: {len(outputs)}/5")
        return outputs

    def run_pfd_processing(self, input_h5: Path, output_h5: Path):
        """è¿è¡ŒPFDå¤„ç†ï¼ˆç›´æ¥è°ƒç”¨ï¼‰"""
        print(f"  ğŸ”§ Running PFD processing...")
        try:
            success = self.pfd_processor.process_single_file(input_h5, output_h5, file_idx=0)
            if success:
                print(f"  âœ… PFD processing completed")
            else:
                print(f"  âŒ PFD processing failed")
        except Exception as e:
            print(f"  âŒ PFD processing failed: {e}")

    def run_efr_processing(self, input_h5: Path, output_h5: Path):
        """è¿è¡ŒEFRå¤„ç†ï¼ˆç›´æ¥è°ƒç”¨ï¼‰"""
        print(f"  ğŸ”§ Running EFR processing...")
        print(f"    Input: {input_h5.name} ({input_h5.stat().st_size/1024/1024:.1f}MB)")
        try:
            success = self.efr_processor.process_single_file(input_h5, output_h5, file_idx=0)
            if success and output_h5.exists():
                output_size = output_h5.stat().st_size / 1024 / 1024
                print(f"  âœ… EFR processing completed - Output: {output_size:.1f}MB")
                if output_size < 0.1:  # Less than 100KB is suspicious
                    print(f"  âš ï¸  Warning: EFR output file is unusually small!")
            else:
                print(f"  âŒ EFR processing failed")
        except Exception as e:
            print(f"  âŒ EFR processing failed: {e}")
            import traceback
            traceback.print_exc()

    def run_baseline_processing(self, input_h5: Path, output_h5: Path):
        """è¿è¡ŒBaselineï¼ˆç¼–è§£ç onlyï¼‰å¤„ç†ï¼ˆç›´æ¥å®ç°ï¼‰"""
        print(f"  ğŸ”§ Running Baseline processing...")
        try:
            # Baseline: Events â†’ Voxel â†’ Events (æµ‹è¯•ç¼–è§£ç ä¿çœŸåº¦)
            events_np = load_h5_events(str(input_h5))

            # Encode
            voxel = events_to_voxel(
                events_np,
                num_bins=8,
                sensor_size=(480, 640),
                fixed_duration_us=100000  # 100ms
            )

            # Decode
            output_events = voxel_to_events(
                voxel,
                total_duration=100000,
                sensor_size=(480, 640)
            )

            # Save to H5
            self.save_h5_events(output_events, output_h5)
            print(f"  âœ… Baseline processing completed")
        except Exception as e:
            print(f"  âŒ Baseline processing failed: {e}")

    def generate_visualizations(self, base_filename: str,
                               input_h5: Path,
                               unet_outputs: dict,
                               pfd_h5: Path,
                               efr_h5: Path,
                               baseline_h5: Path):
        """
        ç”Ÿæˆæ‰€æœ‰æ–¹æ³•çš„å¯è§†åŒ–ï¼ˆåŒä¸€è¾“å…¥çš„æ‰€æœ‰ç»“æœæ”¾åœ¨åŒä¸€å­æ–‡ä»¶å¤¹ï¼‰

        Args:
            unet_outputs: {variant_name: h5_path} å­—å…¸
        """
        # åˆ›å»ºå­æ–‡ä»¶å¤¹ï¼ˆä½¿ç”¨æ–‡ä»¶åŸºç¡€åï¼‰
        vis_subdir = self.visualize_dir / Path(base_filename).stem
        vis_subdir.mkdir(parents=True, exist_ok=True)

        print(f"    ğŸ¬ Generating visualizations to: {vis_subdir.name}/")

        # å®šä¹‰æ‰€æœ‰éœ€è¦å¯è§†åŒ–çš„æ–‡ä»¶
        vis_tasks = [(input_h5, "input")]

        # æ·»åŠ æ‰€æœ‰UNetå˜ä½“
        for variant, h5_path in unet_outputs.items():
            vis_tasks.append((h5_path, f"unet_{variant}"))

        # æ·»åŠ å…¶ä»–æ–¹æ³•
        vis_tasks.extend([
            (pfd_h5, "pfd_output"),
            (efr_h5, "efr_output"),
            (baseline_h5, "baseline_output")
        ])

        for h5_file, method_name in vis_tasks:
            if h5_file.exists():
                try:
                    output_video = vis_subdir / f"{method_name}.mp4"
                    self.video_generator.process_h5_file(str(h5_file), str(output_video))
                    print(f"      âœ… {method_name}.mp4")
                except Exception as e:
                    print(f"      âŒ {method_name} failed: {e}")

    def process_single_segment(self, source_file: Path, start_time: int) -> bool:
        """
        å¤„ç†å•ä¸ª100msæ®µï¼ˆå®Œæ•´æµç¨‹ï¼‰

        Args:
            source_file: æºH5æ–‡ä»¶è·¯å¾„
            start_time: èµ·å§‹æ—¶é—´ï¼ˆå¾®ç§’ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸå¤„ç†
        """
        print(f"  â±ï¸  Processing segment: {start_time/1000:.1f}ms - {(start_time+100000)/1000:.1f}ms")

        # Step 1: å†…å­˜å®‰å…¨åœ°æå–100msæ®µ
        events_segment = self.extract_100ms_segment_safe(source_file, start_time)

        if len(events_segment) == 0:
            print(f"    âŒ No events in segment, skipping...")
            return False

        # Step 2: ç”Ÿæˆæ–‡ä»¶åå¹¶ä¿å­˜åˆ°input
        filename = self.generate_filename(source_file, start_time)
        input_h5 = self.input_dir / filename

        print(f"    ğŸ’¾ Saving to: {filename}")
        self.save_h5_events(events_segment, input_h5)

        # Step 3: è¿è¡Œæ‰€æœ‰å¤„ç†æ–¹æ³•
        print(f"    ğŸ”„ Processing with all methods...")

        # UNet3D (æ‰€æœ‰5ä¸ªå˜ä½“)
        print(f"    ğŸ§  Running all UNet variants (5 models)...")
        unet_outputs = self.run_all_unet_variants(input_h5, filename)

        # PFD
        pfd_h5 = self.inputpfds_dir / filename
        self.run_pfd_processing(input_h5, pfd_h5)

        # EFR
        efr_h5 = self.inputefr_dir / filename
        self.run_efr_processing(input_h5, efr_h5)

        # Baseline
        baseline_h5 = self.outputbaseline_dir / filename
        self.run_baseline_processing(input_h5, baseline_h5)

        # Step 4: ç”Ÿæˆå¯è§†åŒ–
        print(f"    ğŸ“Š Generating visualizations...")
        self.generate_visualizations(
            filename, input_h5, unet_outputs, pfd_h5, efr_h5, baseline_h5
        )

        print(f"    âœ… Segment completed: {filename}")
        return True

    def generate_batch_sequential(self):
        """
        é¡ºåºæ‰¹é‡ç”ŸæˆDSECæ ·æœ¬ï¼ˆå¸¦æ–­ç‚¹ç»­å­˜ï¼‰

        å¤„ç†æµç¨‹ï¼š
        1. æŒ‰æ–‡ä»¶åæ’åºéå†æ‰€æœ‰é•¿ç‚«å…‰æ–‡ä»¶
        2. æ¯ä¸ªæ–‡ä»¶å†…æŒ‰æ—¶é—´é¡ºåºé‡‡æ ·ï¼ˆé—´éš”400msï¼‰
        3. è‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„æ®µï¼ˆæ–­ç‚¹ç»­å­˜ï¼‰
        """
        print(f"\nğŸš€ Starting sequential batch generation with checkpoint resume")
        print("="*80)

        # Step 1: è·å–æ’åºåçš„æ–‡ä»¶åˆ—è¡¨
        flare_files = self.get_sorted_flare_files()

        # Step 2: è§£æå·²æœ‰è¿›åº¦
        progress = self.parse_existing_progress()

        # Step 3: éå†æ¯ä¸ªæ–‡ä»¶
        total_processed = 0
        total_skipped = 0

        for file_idx, source_file in enumerate(flare_files, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ“ File [{file_idx}/{len(flare_files)}]: {source_file.name}")
            print(f"{'='*80}")

            source_name = source_file.stem

            # è·å–æ—¶é—´èŒƒå›´
            try:
                t_min, t_max = self.get_time_range_safe(source_file)
            except Exception as e:
                print(f"  âŒ Failed to read time range: {e}")
                continue

            # ç”Ÿæˆé‡‡æ ·ç‚¹
            time_samples = self.generate_time_samples(t_min, t_max)

            if len(time_samples) == 0:
                print(f"  âš ï¸  No valid time samples, skipping file")
                continue

            # è·å–å·²å¤„ç†çš„æ—¶é—´ç‚¹
            processed_times = progress.get(source_name, set())

            # éå†æ¯ä¸ªé‡‡æ ·ç‚¹
            file_processed = 0
            file_skipped = 0

            for sample_idx, start_time in enumerate(time_samples, 1):
                print(f"\n  [Segment {sample_idx}/{len(time_samples)}]")

                # æ–­ç‚¹ç»­å­˜ï¼šè·³è¿‡å·²å¤„ç†çš„
                if start_time in processed_times:
                    print(f"    â­ï¸  Skipping t={start_time/1000:.1f}ms (already processed)")
                    file_skipped += 1
                    total_skipped += 1
                    continue

                # å¤„ç†æ–°çš„é‡‡æ ·ç‚¹
                try:
                    if self.process_single_segment(source_file, start_time):
                        file_processed += 1
                        total_processed += 1
                        # æ›´æ–°è¿›åº¦ï¼ˆå†…å­˜ä¸­è®°å½•ï¼Œé¿å…é‡å¤å¤„ç†ï¼‰
                        if source_name not in progress:
                            progress[source_name] = set()
                        progress[source_name].add(start_time)
                except Exception as e:
                    print(f"    âŒ Failed to process segment: {e}")
                    import traceback
                    traceback.print_exc()

            print(f"\n  ğŸ“Š File summary: {file_processed} new, {file_skipped} skipped")

        # Final summary
        print("\n" + "="*80)
        print(f"ğŸ‰ Sequential batch generation completed!")
        print(f"ğŸ“Š Total processed: {total_processed} new segments")
        print(f"â­ï¸  Total skipped: {total_skipped} existing segments")
        print(f"ğŸ“‚ Output: {self.output_base}")
        print("="*80)


def main():
    parser = argparse.ArgumentParser(
        description="DSEC Dataset Generator - Sequential processing with checkpoint resume"
    )
    parser.add_argument("--flare_dir", default="/mnt/e/2025/event_flick_flare/main/data/flare_events",
                       help="Flare events directory (WSL format)")
    parser.add_argument("--output_base", default="DSEC_data", help="Output base directory")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")

    args = parser.parse_args()

    generator = DSECDatasetGenerator(
        flare_dir=args.flare_dir,
        output_base=args.output_base,
        debug=args.debug
    )

    # é¡ºåºæ‰¹å¤„ç†ï¼ˆè‡ªåŠ¨æ–­ç‚¹ç»­å­˜ï¼‰
    generator.generate_batch_sequential()


if __name__ == "__main__":
    main()
