#!/usr/bin/env python3
"""
DSEC Dataset Generator - ä»é•¿ç‚«å…‰æ–‡ä»¶ä¸­æ™ºèƒ½æå–100msæ®µå¹¶å¤„ç†

åŸºäºLinuså“²å­¦ï¼š
- æ•°æ®ç»“æ„æ­£ç¡®: éšæœºé€‰æ‹© â†’ å®‰å…¨è¯»å– â†’ 100msæå– â†’ å¤šæ–¹æ³•å¤„ç† â†’ ç»Ÿä¸€å¯è§†åŒ–
- æ¶ˆé™¤ç‰¹æ®Šæƒ…å†µ: ç»Ÿä¸€å¤„ç†æµç¨‹ï¼Œå¤ç”¨ç°æœ‰å·¥å…·
- å®ç”¨ä¸»ä¹‰: å†…å­˜å®‰å…¨ï¼Œé¿å…æº¢å‡º

åŠŸèƒ½ï¼š
1. ä»flare_eventsæ–‡ä»¶å¤¹éšæœºé€‰æ‹©é•¿H5æ–‡ä»¶
2. æ™ºèƒ½è¯»å–ï¼šå…ˆè¯»æ—¶é—´æˆ³ï¼Œå†åªè¯»å–éœ€è¦çš„100msèŒƒå›´ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
3. ä¿å­˜åˆ°DSEC_data/inputï¼ˆå¤ç”¨ç°æœ‰å‘½åæ–¹å¼ï¼‰
4. è¿è¡Œæ‰€æœ‰å¤„ç†æ–¹æ³•ï¼šUNet3D, PFD, Baseline, EFR
5. ç”Ÿæˆå¯è§†åŒ–åˆ°DSEC_data/visualize

Usage:
    python src/tools/generate_dsec_dataset.py --num_samples 5
    python src/tools/generate_dsec_dataset.py --num_samples 10 --debug
"""

import os
import sys
import random
import h5py
import hdf5plugin  # å¿…é¡»importä»¥æ”¯æŒgzipå‹ç¼©çš„H5æ–‡ä»¶
import numpy as np
from pathlib import Path
from datetime import datetime
import subprocess  # ä»…UNet3D inferenceéœ€è¦
import argparse
from typing import Tuple, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.tools.event_video_generator import EventVideoGenerator
from src.data_processing.encode import load_h5_events, events_to_voxel
from src.data_processing.decode import voxel_to_events

# å¯¼å…¥å¤„ç†å™¨ç±»
sys.path.append(str(PROJECT_ROOT / 'ext' / 'PFD'))
sys.path.append(str(PROJECT_ROOT / 'ext' / 'EFR-main'))
from batch_pfd_processor import BatchPFDProcessor
from batch_efr_processor import BatchEFRProcessor


class DSECDatasetGenerator:
    """DSECæ•°æ®é›†ç”Ÿæˆå™¨ - å†…å­˜å®‰å…¨çš„100msæ®µæå–ä¸å¤„ç†"""

    def __init__(self,
                 flare_dir: str = "/mnt/e/2025/event_flick_flare/main/data/flare_events",
                 output_base: str = "DSEC_data",
                 debug: bool = False):
        """
        Args:
            flare_dir: é•¿ç‚«å…‰æ–‡ä»¶ç›®å½•ï¼ˆWSLæ ¼å¼ï¼‰
            output_base: DSEC_dataåŸºç¡€ç›®å½•
            debug: æ˜¯å¦å¯ç”¨debugæ¨¡å¼
        """
        self.flare_dir = Path(flare_dir)
        self.output_base = Path(output_base)
        self.debug = debug

        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.input_dir = self.output_base / "input"
        self.inputpfds_dir = self.output_base / "inputpfds"
        self.output_dir = self.output_base / "output"
        self.outputbaseline_dir = self.output_base / "outputbaseline"
        self.inputefr_dir = self.output_base / "inputefr"  # æ–°å¢EFR
        self.visualize_dir = self.output_base / "visualize"

        # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
        for dir_path in [self.input_dir, self.inputpfds_dir, self.output_dir,
                         self.outputbaseline_dir, self.inputefr_dir, self.visualize_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # è§†é¢‘ç”Ÿæˆå™¨
        self.video_generator = EventVideoGenerator(
            sensor_size=(480, 640),
            frame_duration_ms=2.5,
            fps=10
        )

        # åˆå§‹åŒ–å¤„ç†å™¨
        self.pfd_processor = BatchPFDProcessor(debug=False)
        self.efr_processor = BatchEFRProcessor(debug=False)

        print(f"ğŸš€ DSEC Dataset Generator initialized")
        print(f"ğŸ“‚ Flare source: {self.flare_dir}")
        print(f"ğŸ“‚ Output base: {self.output_base}")

    def get_random_flare_file(self) -> Path:
        """éšæœºé€‰æ‹©ä¸€ä¸ªç‚«å…‰æ–‡ä»¶"""
        flare_files = list(self.flare_dir.glob("*.h5"))
        if not flare_files:
            raise FileNotFoundError(f"No H5 files found in {self.flare_dir}")

        selected = random.choice(flare_files)
        print(f"ğŸ“„ Selected flare file: {selected.name}")
        return selected

    def get_time_range_safe(self, file_path: Path) -> Tuple[int, int]:
        """å®‰å…¨è·å–H5æ–‡ä»¶çš„æ—¶é—´èŒƒå›´ï¼ˆä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰"""
        with h5py.File(file_path, 'r') as f:
            t_data = f['events']['t']
            # åªè¯»å–é¦–å°¾å…ƒç´ æ¥ç¡®å®šæ—¶é—´èŒƒå›´
            t_min = int(t_data[0])
            t_max = int(t_data[-1])

        print(f"  Time range: {t_min/1000:.1f}ms - {t_max/1000:.1f}ms (duration: {(t_max-t_min)/1000:.1f}ms)")
        return t_min, t_max

    def extract_100ms_segment_safe(self, file_path: Path, start_time_us: int) -> np.ndarray:
        """
        å†…å­˜å®‰å…¨åœ°æå–100msäº‹ä»¶æ®µ

        æ ¸å¿ƒç­–ç•¥ï¼šå…ˆè¯»æ—¶é—´æˆ³æ•°ç»„ï¼Œæ‰¾åˆ°ç´¢å¼•èŒƒå›´ï¼Œå†åªè¯»å–è¯¥èŒƒå›´çš„æ‰€æœ‰æ•°æ®
        """
        segment_duration_us = 100000  # 100ms
        end_time_us = start_time_us + segment_duration_us

        with h5py.File(file_path, 'r') as f:
            events_group = f['events']

            # Step 1: åªè¯»å–æ—¶é—´æˆ³æ•°ç»„æ¥ç¡®å®šç´¢å¼•èŒƒå›´
            t_all = events_group['t'][:]

            # Step 2: ä½¿ç”¨å¸ƒå°”ç´¢å¼•æ‰¾åˆ°100msèŒƒå›´å†…çš„äº‹ä»¶ç´¢å¼•
            mask = (t_all >= start_time_us) & (t_all < end_time_us)
            indices = np.where(mask)[0]

            if len(indices) == 0:
                print(f"  âš ï¸  No events in selected time window")
                return np.empty((0, 4))

            # Step 3: åªè¯»å–è¿™ä¸ªèŒƒå›´çš„æ•°æ®ï¼ˆå†…å­˜å®‰å…¨ï¼‰
            idx_start = indices[0]
            idx_end = indices[-1] + 1

            t = events_group['t'][idx_start:idx_end]
            x = events_group['x'][idx_start:idx_end]
            y = events_group['y'][idx_start:idx_end]
            p = events_group['p'][idx_start:idx_end]

            # ææ€§è½¬æ¢ï¼ˆç»Ÿä¸€ä¸º-1/1æ ¼å¼ï¼‰
            p_converted = np.where(p == 1, 1, -1)

            # ç»„åˆæˆ(N,4)æ ¼å¼
            events_segment = np.column_stack((t, x, y, p_converted))

        print(f"  âœ… Extracted {len(events_segment):,} events from segment")
        return events_segment

    def save_h5_events(self, events: np.ndarray, output_path: Path):
        """ä¿å­˜äº‹ä»¶åˆ°H5æ–‡ä»¶ï¼ˆæ ‡å‡†DSECæ ¼å¼ï¼‰"""
        with h5py.File(output_path, 'w') as f:
            events_group = f.create_group('events')
            events_group.create_dataset('t', data=events[:, 0].astype(np.int64),
                                       compression='gzip', compression_opts=9)
            events_group.create_dataset('x', data=events[:, 1].astype(np.uint16),
                                       compression='gzip', compression_opts=9)
            events_group.create_dataset('y', data=events[:, 2].astype(np.uint16),
                                       compression='gzip', compression_opts=9)
            events_group.create_dataset('p', data=events[:, 3].astype(np.int8),
                                       compression='gzip', compression_opts=9)

    def generate_filename(self, source_file: Path, start_time_us: int) -> str:
        """
        ç”ŸæˆDSECæ ‡å‡†æ–‡ä»¶å

        æ ¼å¼: real_flare_{source}_t{time}ms_{datetime}.h5
        """
        source_name = source_file.stem  # ä¾‹å¦‚ï¼šzurich_city_03_a
        time_ms = int(start_time_us / 1000)
        datetime_str = datetime.now().strftime("%Y%m%d_%H%M%S")

        filename = f"real_flare_{source_name}_t{time_ms}ms_{datetime_str}.h5"
        return filename

    def run_unet_inference(self, input_h5: Path, output_h5: Path):
        """è¿è¡ŒUNet3Dæ¨ç†"""
        cmd = [
            sys.executable, "main.py", "inference",
            "--config", "configs/inference_config.yaml",
            "--input", str(input_h5),
            "--output", str(output_h5)
        ]

        print(f"  ğŸ”§ Running UNet3D inference...")
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=PROJECT_ROOT)

        if result.returncode == 0:
            print(f"  âœ… UNet3D inference completed")
        else:
            print(f"  âŒ UNet3D inference failed: {result.stderr}")

    def run_pfd_processing(self, input_h5: Path, output_h5: Path):
        """è¿è¡ŒPFDå¤„ç†ï¼ˆç›´æ¥è°ƒç”¨ï¼‰"""
        print(f"  ğŸ”§ Running PFD processing...")
        try:
            success = self.pfd_processor.process_single_file(input_h5, output_h5, file_idx=0)
            if success:
                print(f"  âœ… PFD processing completed")
            else:
                print(f"  âŒ PFD processing failed")
        except Exception as e:
            print(f"  âŒ PFD processing failed: {e}")

    def run_efr_processing(self, input_h5: Path, output_h5: Path):
        """è¿è¡ŒEFRå¤„ç†ï¼ˆç›´æ¥è°ƒç”¨ï¼‰"""
        print(f"  ğŸ”§ Running EFR processing...")
        print(f"    Input: {input_h5.name} ({input_h5.stat().st_size/1024/1024:.1f}MB)")
        try:
            success = self.efr_processor.process_single_file(input_h5, output_h5, file_idx=0)
            if success and output_h5.exists():
                output_size = output_h5.stat().st_size / 1024 / 1024
                print(f"  âœ… EFR processing completed - Output: {output_size:.1f}MB")
                if output_size < 0.1:  # Less than 100KB is suspicious
                    print(f"  âš ï¸  Warning: EFR output file is unusually small!")
            else:
                print(f"  âŒ EFR processing failed")
        except Exception as e:
            print(f"  âŒ EFR processing failed: {e}")
            import traceback
            traceback.print_exc()

    def run_baseline_processing(self, input_h5: Path, output_h5: Path):
        """è¿è¡ŒBaselineï¼ˆç¼–è§£ç onlyï¼‰å¤„ç†ï¼ˆç›´æ¥å®ç°ï¼‰"""
        print(f"  ğŸ”§ Running Baseline processing...")
        try:
            # Baseline: Events â†’ Voxel â†’ Events (æµ‹è¯•ç¼–è§£ç ä¿çœŸåº¦)
            events_np = load_h5_events(str(input_h5))

            # Encode
            voxel = events_to_voxel(
                events_np,
                num_bins=8,
                sensor_size=(480, 640),
                fixed_duration_us=100000  # 100ms
            )

            # Decode
            output_events = voxel_to_events(
                voxel,
                total_duration=100000,
                sensor_size=(480, 640)
            )

            # Save to H5
            self.save_h5_events(output_events, output_h5)
            print(f"  âœ… Baseline processing completed")
        except Exception as e:
            print(f"  âŒ Baseline processing failed: {e}")

    def generate_visualizations(self, base_filename: str,
                               input_h5: Path,
                               unet_h5: Path,
                               pfd_h5: Path,
                               efr_h5: Path,
                               baseline_h5: Path):
        """ç”Ÿæˆæ‰€æœ‰æ–¹æ³•çš„å¯è§†åŒ–ï¼ˆåŒä¸€è¾“å…¥çš„æ‰€æœ‰ç»“æœæ”¾åœ¨åŒä¸€å­æ–‡ä»¶å¤¹ï¼‰"""
        # åˆ›å»ºå­æ–‡ä»¶å¤¹ï¼ˆä½¿ç”¨æ–‡ä»¶åŸºç¡€åï¼‰
        vis_subdir = self.visualize_dir / Path(base_filename).stem
        vis_subdir.mkdir(parents=True, exist_ok=True)

        print(f"  ğŸ¬ Generating visualizations to: {vis_subdir.name}/")

        # å®šä¹‰æ‰€æœ‰éœ€è¦å¯è§†åŒ–çš„æ–‡ä»¶
        vis_tasks = [
            (input_h5, "input"),
            (unet_h5, "unet_output"),
            (pfd_h5, "pfd_output"),
            (efr_h5, "efr_output"),
            (baseline_h5, "baseline_output")
        ]

        for h5_file, method_name in vis_tasks:
            if h5_file.exists():
                try:
                    output_video = vis_subdir / f"{method_name}.mp4"
                    self.video_generator.process_h5_file(str(h5_file), str(output_video))
                    print(f"    âœ… {method_name}.mp4 generated")
                except Exception as e:
                    print(f"    âŒ {method_name} visualization failed: {e}")

    def generate_single_sample(self):
        """ç”Ÿæˆå•ä¸ªDSECæ ·æœ¬ï¼ˆå®Œæ•´æµç¨‹ï¼‰"""
        print("\n" + "="*80)
        print("ğŸ¯ Generating new DSEC sample...")

        # Step 1: éšæœºé€‰æ‹©ç‚«å…‰æ–‡ä»¶
        source_file = self.get_random_flare_file()

        # Step 2: å®‰å…¨è·å–æ—¶é—´èŒƒå›´
        t_min, t_max = self.get_time_range_safe(source_file)

        # Step 3: éšæœºé€‰æ‹©100msèµ·å§‹æ—¶é—´
        max_start = t_max - 100000  # ç¡®ä¿æœ‰å®Œæ•´çš„100ms
        if max_start <= t_min:
            print(f"  âš ï¸  File too short, using entire duration")
            start_time = t_min
        else:
            start_time = random.randint(t_min, max_start)

        print(f"  ğŸ² Random start time: {start_time/1000:.1f}ms")

        # Step 4: å†…å­˜å®‰å…¨åœ°æå–100msæ®µ
        events_segment = self.extract_100ms_segment_safe(source_file, start_time)

        if len(events_segment) == 0:
            print(f"  âŒ No events in segment, skipping...")
            return False

        # Step 5: ç”Ÿæˆæ–‡ä»¶åå¹¶ä¿å­˜åˆ°input
        filename = self.generate_filename(source_file, start_time)
        input_h5 = self.input_dir / filename

        print(f"  ğŸ’¾ Saving to: {filename}")
        self.save_h5_events(events_segment, input_h5)

        # Step 6: è¿è¡Œæ‰€æœ‰å¤„ç†æ–¹æ³•
        print(f"\n  ğŸ”„ Processing with all methods...")

        # UNet3D
        unet_h5 = self.output_dir / filename
        self.run_unet_inference(input_h5, unet_h5)

        # PFD
        pfd_h5 = self.inputpfds_dir / filename
        self.run_pfd_processing(input_h5, pfd_h5)

        # EFR (æ–°å¢)
        efr_h5 = self.inputefr_dir / filename
        self.run_efr_processing(input_h5, efr_h5)

        # Baseline
        baseline_h5 = self.outputbaseline_dir / filename
        self.run_baseline_processing(input_h5, baseline_h5)

        # Step 7: ç”Ÿæˆå¯è§†åŒ–
        print(f"\n  ğŸ“Š Generating visualizations...")
        self.generate_visualizations(
            filename, input_h5, unet_h5, pfd_h5, efr_h5, baseline_h5
        )

        print(f"\nâœ… Sample generation completed: {filename}")
        return True

    def generate_batch(self, num_samples: int):
        """æ‰¹é‡ç”ŸæˆDSECæ ·æœ¬"""
        print(f"\nğŸš€ Starting batch generation: {num_samples} samples")
        print("="*80)

        success_count = 0
        for i in range(num_samples):
            print(f"\n[Sample {i+1}/{num_samples}]")
            if self.generate_single_sample():
                success_count += 1

        print("\n" + "="*80)
        print(f"ğŸ‰ Batch generation completed!")
        print(f"ğŸ“Š Success: {success_count}/{num_samples} samples")
        print(f"ğŸ“‚ Output: {self.output_base}")


def main():
    parser = argparse.ArgumentParser(description="DSEC Dataset Generator - Memory-safe 100ms extraction and processing")
    parser.add_argument("--num_samples", type=int, default=1, help="Number of samples to generate")
    parser.add_argument("--flare_dir", default="/mnt/e/2025/event_flick_flare/main/data/flare_events",
                       help="Flare events directory (WSL format)")
    parser.add_argument("--output_base", default="DSEC_data", help="Output base directory")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")

    args = parser.parse_args()

    generator = DSECDatasetGenerator(
        flare_dir=args.flare_dir,
        output_base=args.output_base,
        debug=args.debug
    )

    generator.generate_batch(args.num_samples)


if __name__ == "__main__":
    main()
