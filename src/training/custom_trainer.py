"""
Custom Training System for Event-Voxel Denoising
åªä½¿ç”¨pytorch-3dunetçš„UNet3Dæ¨¡å‹ï¼Œè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import logging
from pathlib import Path
import time
from typing import Dict, Any, Optional
import json
from tqdm import tqdm

class EventVoxelTrainer:
    """
    è‡ªå®šä¹‰è®­ç»ƒå™¨ï¼Œåªä½¿ç”¨pytorch-3dunetçš„UNet3Dæ¨¡å‹
    å®ç°å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€checkpointing
    """
    
    def __init__(self, 
                 model: nn.Module,
                 train_loader: DataLoader,
                 val_loader: DataLoader,
                 config: Dict[str, Any],
                 device: str = 'cuda'):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: pytorch-3dunetçš„UNet3Dæ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨  
            config: è®­ç»ƒé…ç½®å­—å…¸
            device: è®­ç»ƒè®¾å¤‡
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = device
        
        # è®¾ç½®logging
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self._setup_optimizer_and_loss()
        
        # åˆå§‹åŒ–è°ƒåº¦å™¨
        self._setup_scheduler()
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.current_epoch = 0
        self.current_iteration = 0
        self.best_val_loss = float('inf')
        
        # è®¾ç½®checkpointç›®å½•
        self.checkpoint_dir = Path(config['trainer']['checkpoint_dir'])
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®tensorboard
        if config.get('logger', {}).get('name') == 'TensorBoardLogger':
            log_dir = config['logger']['log_dir']
            self.writer = SummaryWriter(log_dir)
        else:
            self.writer = None
        
        self.logger.info(f"EventVoxelTrainer initialized:")
        self.logger.info(f"  Device: {device}")
        self.logger.info(f"  Model: {model.__class__.__name__}")
        self.logger.info(f"  Train samples: {len(train_loader.dataset)}")
        self.logger.info(f"  Val samples: {len(val_loader.dataset)}")
        self.logger.info(f"  Checkpoint dir: {self.checkpoint_dir}")
        
        # æ·»åŠ æ¨¡å‹æ¶æ„æ£€æŸ¥
        self._debug_model_architecture()
    
    def _debug_model_architecture(self):
        """è°ƒè¯•æ¨¡å‹æ¶æ„ï¼Œæ£€æŸ¥å¯èƒ½å¯¼è‡´è¾“å‡ºå…¨1çš„é—®é¢˜"""
        self.logger.info("=== Model Architecture Debug ===")
        
        # æ£€æŸ¥final_sigmoidè®¾ç½®
        if hasattr(self.model, 'final_sigmoid'):
            self.logger.info(f"Model final_sigmoid: {self.model.final_sigmoid}")
        
        # æ£€æŸ¥æ¨¡å‹çš„æœ€åå‡ å±‚
        model_children = list(self.model.children())
        if model_children:
            last_layer = model_children[-1]
            self.logger.info(f"Last layer type: {type(last_layer)}")
            
        # å¿«é€Ÿå‰å‘ä¼ æ’­æµ‹è¯•
        self.model.eval()
        with torch.no_grad():
            # åˆ›å»ºæµ‹è¯•è¾“å…¥: å…¨0
            test_input_zeros = torch.zeros(1, 1, 8, 480, 640).to(self.device)
            test_output_zeros = self.model(test_input_zeros)
            self.logger.info(f"Test with zeros input: output_mean={test_output_zeros.mean():.6f}, output_std={test_output_zeros.std():.6f}")
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥: å…¨1
            test_input_ones = torch.ones(1, 1, 8, 480, 640).to(self.device)
            test_output_ones = self.model(test_input_ones)
            self.logger.info(f"Test with ones input: output_mean={test_output_ones.mean():.6f}, output_std={test_output_ones.std():.6f}")
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥: éšæœº
            test_input_random = torch.randn(1, 1, 8, 480, 640).to(self.device)
            test_output_random = self.model(test_input_random)
            self.logger.info(f"Test with random input: output_mean={test_output_random.mean():.6f}, output_std={test_output_random.std():.6f}")
        
        self.model.train()
        self.logger.info("=== End Model Debug ===")
    
    def _setup_optimizer_and_loss(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°"""
        opt_config = self.config['optimizer']
        
        if opt_config['name'] == 'Adam':
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=opt_config['learning_rate'],
                weight_decay=opt_config.get('weight_decay', 0)
            )
        elif opt_config['name'] == 'SGD':
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=opt_config['learning_rate'],
                momentum=opt_config.get('momentum', 0.9),
                weight_decay=opt_config.get('weight_decay', 0)
            )
        else:
            raise ValueError(f"Unsupported optimizer: {opt_config['name']}")
        
        # æŸå¤±å‡½æ•°
        loss_config = self.config['loss']
        if loss_config['name'] == 'MSELoss':
            self.criterion = nn.MSELoss()
        elif loss_config['name'] == 'L1Loss':
            self.criterion = nn.L1Loss()
        elif loss_config['name'] == 'SmoothL1Loss':
            self.criterion = nn.SmoothL1Loss()
        else:
            raise ValueError(f"Unsupported loss: {loss_config['name']}")
        
        self.logger.info(f"Optimizer: {opt_config['name']} (LR: {opt_config['learning_rate']})")
        self.logger.info(f"Loss function: {loss_config['name']}")
    
    def _setup_scheduler(self):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if 'lr_scheduler' not in self.config:
            self.scheduler = None
            return
        
        sched_config = self.config['lr_scheduler']
        
        if sched_config['name'] == 'MultiStepLR':
            self.scheduler = optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=sched_config['milestones'],
                gamma=sched_config['gamma']
            )
        elif sched_config['name'] == 'StepLR':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=sched_config['step_size'],
                gamma=sched_config['gamma']
            )
        else:
            self.scheduler = None
            self.logger.warning(f"Unsupported scheduler: {sched_config['name']}")
        
        if self.scheduler:
            self.logger.info(f"Scheduler: {sched_config['name']}")
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        # åˆ›å»ºtqdmè¿›åº¦æ¡
        progress_bar = tqdm(
            enumerate(self.train_loader), 
            total=len(self.train_loader),
            desc=f"Epoch {self.current_epoch + 1}",
            leave=False,
            ncols=100
        )
        
        for batch_idx, batch in progress_bar:
            # æ£€æŸ¥debugæ¨¡å¼æ˜¯å¦éœ€è¦æå‰é€€å‡º
            debug_config = self.config.get('debug', {})
            if debug_config.get('enabled', False):
                max_iterations = debug_config.get('max_iterations', 2)
                if batch_idx >= max_iterations:
                    self.logger.info(f"ğŸ› DEBUG MODE: Stopping after {max_iterations} iterations")
                    break
            
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = batch['raw'].to(self.device)      # (B, 1, 8, H, W) 
            targets = batch['label'].to(self.device)   # (B, 1, 8, H, W)
            
            # Debugå¯è§†åŒ–é’©å­ - åœ¨æ•°æ®äº§ç”Ÿçš„åœ°æ–¹è§¦å‘
            if debug_config.get('enabled', False) and batch_idx < 2:  # åªå¯¹å‰2ä¸ªbatchåšå¯è§†åŒ–
                self._trigger_debug_visualization(
                    batch_idx, inputs, targets, batch, 
                    debug_config['debug_dir'], self.current_epoch
                )
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(inputs)               # (B, 1, 8, H, W)
            
            # Debugå¯è§†åŒ–é’©å­ - æ¨¡å‹è¾“å‡ºå
            if debug_config.get('enabled', False) and batch_idx < 2:
                self._trigger_model_output_visualization(
                    batch_idx, outputs, debug_config['debug_dir'], self.current_epoch
                )
            
            # æ·±åº¦è°ƒè¯•è®­ç»ƒé˜¶æ®µ (æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡)
            if batch_idx % 100 == 0:
                print(f"\n[DEBUG-TRAIN] Batch {batch_idx}: Input mean={inputs.mean():.4f}, Output mean={outputs.mean():.4f}")
                print(f"[DEBUG-TRAIN] Are outputs identical to inputs? {torch.equal(outputs, inputs)}")
                print(f"[DEBUG-TRAIN] Model in training mode? {self.model.training}")
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            num_batches += 1
            self.current_iteration += 1
            
            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            avg_loss_so_far = total_loss / num_batches
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Avg': f'{avg_loss_so_far:.4f}',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
            
            # Tensorboardè®°å½•
            if self.writer:
                self.writer.add_scalar('Loss/Train_Batch', loss.item(), self.current_iteration)
                self.writer.add_scalar('LR', self.optimizer.param_groups[0]['lr'], self.current_iteration)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ (æ¯Nä¸ªiteration)
            trainer_config = self.config['trainer']
            validate_after_iters = trainer_config.get('validate_after_iters', 100)
            
            if self.current_iteration % validate_after_iters == 0:
                # è°ƒè¯•è¾“å‡ºéªŒè¯è§¦å‘
                print(f"\n[DEBUG] Validation triggered: iter={self.current_iteration}, validate_after_iters={validate_after_iters}", flush=True)
                
                # æš‚åœè®­ç»ƒè¿›åº¦æ¡ï¼Œè¿›è¡ŒéªŒè¯
                progress_bar.set_description(f"Epoch {self.current_epoch + 1} (Validating...)")
                
                # æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦åœ¨å˜åŒ– (è°ƒè¯•)
                model_params_sum = sum([p.sum().item() for p in self.model.parameters()])
                print(f"[DEBUG] Model params sum: {model_params_sum:.6f}", flush=True)
                
                # æ‰§è¡ŒéªŒè¯
                val_metrics = self.validate_epoch()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                is_best = val_metrics['loss'] < self.best_val_loss
                if is_best:
                    self.best_val_loss = val_metrics['loss']
                
                # ä¿å­˜checkpoint (æ·»åŠ è°ƒè¯•ä¿¡æ¯)
                try:
                    self.save_checkpoint(is_best=is_best)
                    checkpoint_name = f"epoch_{self.current_epoch:04d}_iter_{self.current_iteration:06d}"
                    checkpoint_status = f"âœ…({checkpoint_name})" 
                except Exception as e:
                    checkpoint_status = f"âŒ({e})"
                
                # è¾“å‡ºéªŒè¯ç»“æœ (å¼ºåˆ¶åˆ·æ–°è¾“å‡º)
                best_indicator = " ğŸ¯" if is_best else ""
                result_msg = f"\nğŸ’¯ Iter {self.current_iteration:4d}: Val={val_metrics['loss']:.4f}{best_indicator} {checkpoint_status}"
                print(result_msg, flush=True)
                
                # æ¢å¤è®­ç»ƒè¿›åº¦æ¡æè¿°
                progress_bar.set_description(f"Epoch {self.current_epoch + 1}")
            
            # æ—©æœŸåœæ­¢æ£€æŸ¥ï¼ˆåŸºäºè¿­ä»£æ•°ï¼‰
            max_iters = trainer_config.get('max_num_iterations')
            if max_iters and self.current_iteration >= max_iters:
                progress_bar.set_description(f"Epoch {self.current_epoch + 1} (Max iters reached)")
                break
        
        progress_bar.close()
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        return {'loss': avg_loss, 'num_batches': num_batches}
    
    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        
        # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡ï¼ˆå¿«é€ŸéªŒè¯æ¨¡å¼ï¼‰
        # 50ä¸ªtestæ–‡ä»¶ Ã— 5ä¸ªsegments = 250ä¸ªéªŒè¯æ ·æœ¬
        # åªéªŒè¯å‰2ä¸ªæ–‡ä»¶ Ã— 5ä¸ªsegments = 10ä¸ªæ ·æœ¬
        max_val_batches = 10  # å‰2ä¸ªæ–‡ä»¶çš„10ä¸ªæ•°æ®å¯¹
        
        # ä¿®å¤éªŒè¯é€»è¾‘ï¼šä¸è½¬æ¢ä¸ºlistï¼Œç›´æ¥éå†å¹¶è®¡æ•°
        batch_losses = []  # è®°å½•æ¯ä¸ªbatchçš„loss
        with torch.no_grad():
            for batch_idx, batch in enumerate(self.val_loader):
                if batch_idx >= max_val_batches:
                    break
                inputs = batch['raw'].to(self.device)
                targets = batch['label'].to(self.device)
                
                # æ·±åº¦è°ƒè¯•ï¼šæ£€æŸ¥æ¨¡å‹è¾“å…¥è¾“å‡º
                if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªbatchæ‰“å°
                    print(f"[DEBUG] Input stats: min={inputs.min():.4f}, max={inputs.max():.4f}, mean={inputs.mean():.4f}")
                    print(f"[DEBUG] Target stats: min={targets.min():.4f}, max={targets.max():.4f}, mean={targets.mean():.4f}")
                    # æ£€æŸ¥voxelå€¼åˆ†å¸ƒ
                    unique_values_input = torch.unique(inputs).cpu().numpy()[:10]  # å‰10ä¸ªå”¯ä¸€å€¼
                    unique_values_target = torch.unique(targets).cpu().numpy()[:10]
                    print(f"[DEBUG] Input unique values (first 10): {unique_values_input}")
                    print(f"[DEBUG] Target unique values (first 10): {unique_values_target}")
                
                outputs = self.model(inputs)
                
                if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªbatchæ‰“å°
                    print(f"[DEBUG] Output stats: min={outputs.min():.4f}, max={outputs.max():.4f}, mean={outputs.mean():.4f}")
                    print(f"[DEBUG] Output shape: {outputs.shape}")
                    print(f"[DEBUG] Are outputs identical to targets? {torch.equal(outputs, targets)}")
                    print(f"[DEBUG] Output dtype: {outputs.dtype}, device: {outputs.device}")
                    print(f"[DEBUG] Output requires_grad: {outputs.requires_grad}")
                    
                    # æ£€æŸ¥æ¨¡å‹çš„æœ€åå‡ å±‚è¾“å‡º
                    print(f"[DEBUG] Checking model architecture...")
                
                loss = self.criterion(outputs, targets)
                
                batch_loss = loss.item()
                batch_losses.append(batch_loss)
                total_loss += batch_loss
                num_batches += 1
        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        
        # è°ƒè¯•è¾“å‡ºéªŒè¯ç»Ÿè®¡
        batch_losses_str = ", ".join([f"{loss:.4f}" for loss in batch_losses])
        print(f"[DEBUG] Validation completed: {num_batches} batches, avg_loss={avg_loss:.6f}", flush=True)
        print(f"[DEBUG] Individual batch losses: [{batch_losses_str}]", flush=True)
        print(f"[DEBUG] Input shape: {inputs.shape}, Target shape: {targets.shape}", flush=True)
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼ (å…³é”®ä¿®å¤!)
        self.model.train()
        
        return {'loss': avg_loss, 'num_batches': num_batches}
    
    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'epoch': self.current_epoch,
            'iteration': self.current_iteration,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # ä¿å­˜æœ€æ–°checkpoint
        latest_path = self.checkpoint_dir / 'latest_checkpoint.pth'
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³checkpoint
        if is_best:
            best_path = self.checkpoint_dir / 'best_checkpoint.pth'
            torch.save(checkpoint, best_path)
            # self.logger.info(f"Saved best checkpoint: {best_path}")  # Reduce verbosity
        
        # ä¿å­˜iteration checkpoint (é¿å…è¦†ç›–)
        iter_path = self.checkpoint_dir / f'checkpoint_epoch_{self.current_epoch:04d}_iter_{self.current_iteration:06d}.pth'
        torch.save(checkpoint, iter_path)
        
        # self.logger.info(f"Saved checkpoint: {latest_path}")  # Reduce verbosity
    
    def load_checkpoint(self, checkpoint_path: str) -> bool:
        """åŠ è½½checkpoint"""
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            self.logger.warning(f"Checkpoint not found: {checkpoint_path}")
            return False
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.current_epoch = checkpoint['epoch']
            self.current_iteration = checkpoint['iteration']
            self.best_val_loss = checkpoint['best_val_loss']
            
            if self.scheduler and 'scheduler_state_dict' in checkpoint:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            self.logger.info(f"Loaded checkpoint from epoch {self.current_epoch}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to load checkpoint: {e}")
            return False
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info("=== Starting Event-Voxel Denoising Training ===")
        
        trainer_config = self.config['trainer']
        max_epochs = trainer_config.get('max_num_epochs', 100)
        max_iters = trainer_config.get('max_num_iterations', None)
        validate_after_iters = trainer_config.get('validate_after_iters', 100)
        
        start_time = time.time()
        
        for epoch in range(self.current_epoch, max_epochs):
            self.current_epoch = epoch
            
            # ç®€æ´çš„epochå¼€å§‹ä¿¡æ¯
            print(f"\nğŸ“Š Epoch {epoch + 1}/{max_epochs}")
            
            # è®­ç»ƒ - éªŒè¯é€»è¾‘ç°åœ¨åœ¨train_epochå†…éƒ¨å¤„ç†
            train_metrics = self.train_epoch()
            
            # Epochå®Œæˆåçš„æ€»ç»“ - ç®€åŒ–ç‰ˆæœ¬ï¼Œè¯¦ç»†éªŒè¯åœ¨train_epochå†…éƒ¨
            print(f"âœ… Epoch {epoch + 1:3d}: Train={train_metrics['loss']:.4f}")
            
            # Tensorboard epochçº§è®°å½•
            if self.writer:
                self.writer.add_scalar('Loss/Train_Epoch', train_metrics['loss'], epoch)
            
            # æ¯ä¸ªepochéƒ½ä¿å­˜checkpoint (ä¿è¯ä¸ä¸¢å¤±)
            self.save_checkpoint(is_best=False)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
            
            # æ—©æœŸåœæ­¢æ£€æŸ¥ï¼ˆåŸºäºè¿­ä»£æ•°ï¼‰
            if max_iters and self.current_iteration >= max_iters:
                print(f"ğŸ›‘ Training stopped: reached max iterations {max_iters}")
                # åœ¨è®­ç»ƒç»“æŸå‰ä¿å­˜final checkpoint
                val_metrics = self.validate_epoch()
                is_best = val_metrics['loss'] < self.best_val_loss
                best_indicator = " ğŸ¯" if is_best else ""
                print(f"ğŸ Final: Train={train_metrics['loss']:.4f}, Val={val_metrics['loss']:.4f}{best_indicator}")
                if is_best:
                    self.best_val_loss = val_metrics['loss']
                self.save_checkpoint(is_best=is_best)
                break
        
        # è®­ç»ƒç»“æŸ
        total_time = time.time() - start_time
        print(f"\nğŸ‰ Training Completed!")
        print(f"â±ï¸  Total time: {total_time/3600:.2f}h")
        print(f"ğŸ¯ Best val loss: {self.best_val_loss:.4f}")
        print(f"ğŸ’¾ Checkpoint: {self.checkpoint_dir}/best_checkpoint.pth")
        
        # ä¿å­˜è®­ç»ƒæ‘˜è¦
        summary = {
            'total_epochs': self.current_epoch + 1,
            'total_iterations': self.current_iteration,
            'best_val_loss': self.best_val_loss,
            'total_time_hours': total_time / 3600,
            'final_lr': self.optimizer.param_groups[0]['lr']
        }
        
        summary_path = self.checkpoint_dir / 'training_summary.json'
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        if self.writer:
            self.writer.close()
        
        return self.best_val_loss
    
    def _trigger_debug_visualization(self, batch_idx: int, inputs: torch.Tensor, targets: torch.Tensor, 
                                   batch: dict, debug_dir: str, epoch: int):
        """
        Debugå¯è§†åŒ–é’©å­ - åœ¨æ•°æ®äº§ç”Ÿçš„åœ°æ–¹è§¦å‘
        ç”Ÿæˆ9ä¸ªç‹¬ç«‹çš„å¯è§†åŒ–æ–‡ä»¶å¤¹ï¼š
        1. è¾“å…¥äº‹ä»¶3Då¯è§†åŒ–
        2. è¾“å…¥äº‹ä»¶2Då¯è§†åŒ–  
        3. è¾“å…¥voxelå¯è§†åŒ–
        4. çœŸå€¼äº‹ä»¶3Då¯è§†åŒ–
        5. çœŸå€¼äº‹ä»¶2Då¯è§†åŒ–
        6. çœŸå€¼voxelå¯è§†åŒ–
        """
        try:
            import os
            from pathlib import Path
            
            # åˆ›å»ºdebugè¾“å‡ºç»“æ„
            iteration_dir = Path(debug_dir) / f"epoch_{epoch:03d}_iter_{batch_idx:03d}"
            iteration_dir.mkdir(parents=True, exist_ok=True)
            
            self.logger.info(f"ğŸ› Generating 9 debug visualizations for Epoch {epoch}, Batch {batch_idx}")
            self.logger.info(f"ğŸ› Output directory: {iteration_dir}")
            
            # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ– (batch_sizeé€šå¸¸ä¸º1)
            input_voxel = inputs[0, 0].cpu()   # (8, H, W) - å»é™¤batchå’Œchannelç»´åº¦
            target_voxel = targets[0, 0].cpu() # (8, H, W)
            
            # ä»voxelè§£ç å›eventsè¿›è¡Œå¯è§†åŒ–
            from src.data_processing.decode import voxel_to_events
            
            # è§£ç inputå’Œtarget voxelä¸ºevents
            # æ³¨æ„ï¼šéœ€è¦ä½¿ç”¨æ­£ç¡®çš„duration (20ms = 20000us)
            input_events_np = voxel_to_events(input_voxel, total_duration=20000, sensor_size=(480, 640))
            target_events_np = voxel_to_events(target_voxel, total_duration=20000, sensor_size=(480, 640))
            
            # ä½¿ç”¨ä¸“ä¸šå¯è§†åŒ–ç³»ç»Ÿ - ä¿®å¤å‡½æ•°è°ƒç”¨
            from src.data_processing.professional_visualizer import visualize_events, visualize_voxel
            
            # === è¾“å…¥æ•°æ®å¯è§†åŒ– ===
            # 1-2. è¾“å…¥äº‹ä»¶3D+2Då¯è§†åŒ– (ä½¿ç”¨æ­£ç¡®çš„å‚æ•°é¡ºåº)
            input_events_dir = iteration_dir / "1_input_events"
            input_events_dir.mkdir(exist_ok=True)
            visualize_events(input_events_np, sensor_size=(480, 640), output_dir=str(input_events_dir), 
                           name="input_events", num_time_slices=8)
            
            # 3. è¾“å…¥voxelå¯è§†åŒ–
            input_voxel_dir = iteration_dir / "3_input_voxel"
            input_voxel_dir.mkdir(exist_ok=True)
            visualize_voxel(input_voxel, sensor_size=(480, 640), output_dir=str(input_voxel_dir), 
                          name="input_voxel", duration_ms=20)
            
            # === çœŸå€¼æ•°æ®å¯è§†åŒ– ===
            # 4-5. çœŸå€¼äº‹ä»¶3D+2Då¯è§†åŒ– (ä½¿ç”¨æ­£ç¡®çš„å‚æ•°é¡ºåº)
            target_events_dir = iteration_dir / "4_target_events"
            target_events_dir.mkdir(exist_ok=True)
            visualize_events(target_events_np, sensor_size=(480, 640), output_dir=str(target_events_dir), 
                           name="target_events", num_time_slices=8)
            
            # 6. çœŸå€¼voxelå¯è§†åŒ–
            target_voxel_dir = iteration_dir / "6_target_voxel"
            target_voxel_dir.mkdir(exist_ok=True)
            visualize_voxel(target_voxel, sensor_size=(480, 640), output_dir=str(target_voxel_dir), 
                          name="target_voxel", duration_ms=20)
            
            self.logger.info(f"ğŸ› Generated input and target visualizations (1,3,4,6/9) in {iteration_dir}")
            
        except Exception as e:
            self.logger.warning(f"ğŸ› Debug visualization failed: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())
    
    def _trigger_model_output_visualization(self, batch_idx: int, outputs: torch.Tensor, 
                                          debug_dir: str, epoch: int):
        """
        Debugå¯è§†åŒ–é’©å­ - æ¨¡å‹è¾“å‡ºåè§¦å‘
        ç”Ÿæˆ9ä¸ªå¯è§†åŒ–æ–‡ä»¶å¤¹ä¸­çš„å3ä¸ªï¼š
        7. æ¨¡å‹è¾“å‡ºäº‹ä»¶3Då¯è§†åŒ–
        8. æ¨¡å‹è¾“å‡ºäº‹ä»¶2Då¯è§†åŒ–  
        9. æ¨¡å‹è¾“å‡ºvoxelå¯è§†åŒ–
        """
        try:
            from pathlib import Path
            
            iteration_dir = Path(debug_dir) / f"epoch_{epoch:03d}_iter_{batch_idx:03d}"
            
            # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ¨¡å‹è¾“å‡º
            output_voxel = outputs[0, 0].cpu()  # (8, H, W)
            
            # ä»output voxelè§£ç ä¸ºevents
            from src.data_processing.decode import voxel_to_events
            output_events_np = voxel_to_events(output_voxel, total_duration=20000, sensor_size=(480, 640))
            
            # ä½¿ç”¨ä¸“ä¸šå¯è§†åŒ–ç³»ç»Ÿ - ä¿®å¤å‡½æ•°è°ƒç”¨
            from src.data_processing.professional_visualizer import visualize_events, visualize_voxel
            
            # === æ¨¡å‹è¾“å‡ºå¯è§†åŒ– ===
            # 7-8. æ¨¡å‹è¾“å‡ºäº‹ä»¶3D+2Då¯è§†åŒ– (ä½¿ç”¨æ­£ç¡®çš„å‚æ•°é¡ºåº)
            output_events_dir = iteration_dir / "7_output_events"
            output_events_dir.mkdir(exist_ok=True)
            visualize_events(output_events_np, sensor_size=(480, 640), output_dir=str(output_events_dir), 
                           name="output_events", num_time_slices=8)
            
            # 9. æ¨¡å‹è¾“å‡ºvoxelå¯è§†åŒ–
            output_voxel_dir = iteration_dir / "9_output_voxel"
            output_voxel_dir.mkdir(exist_ok=True)
            visualize_voxel(output_voxel, sensor_size=(480, 640), output_dir=str(output_voxel_dir), 
                          name="output_voxel", duration_ms=20)
            
            self.logger.info(f"ğŸ› Generated model output visualizations (7,9/9) in {iteration_dir}")
            self.logger.info(f"ğŸ› All debug visualizations completed! (6 folders total: 1,3,4,6,7,9)")
            
            # ç”Ÿæˆæ¯”è¾ƒæ€»ç»“
            self._generate_debug_summary(iteration_dir, batch_idx, epoch)
            
        except Exception as e:
            self.logger.warning(f"ğŸ› Model output visualization failed: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())
    
    def _generate_debug_summary(self, iteration_dir: Path, batch_idx: int, epoch: int):
        """ç”Ÿæˆdebugæ€»ç»“ä¿¡æ¯"""
        try:
            summary_file = iteration_dir / "debug_summary.txt"
            
            with open(summary_file, 'w') as f:
                f.write(f"Debug Visualization Summary\n")
                f.write(f"========================\n")
                f.write(f"Epoch: {epoch}\n")
                f.write(f"Batch: {batch_idx}\n")
                f.write(f"Model: {self.model.__class__.__name__}\n")
                f.write(f"Device: {self.device}\n")
                f.write(f"\n6 Visualization Folders (comprehensive):\n")
                f.write(f"1. 1_input_events/          - Input events (3D+2D+temporal) comprehensive\n")
                f.write(f"2. 3_input_voxel/           - Input voxel temporal bins\n")
                f.write(f"3. 4_target_events/         - Target events (3D+2D+temporal) comprehensive\n") 
                f.write(f"4. 6_target_voxel/          - Target voxel temporal bins\n")
                f.write(f"5. 7_output_events/         - Model output events (3D+2D+temporal) comprehensive\n")
                f.write(f"6. 9_output_voxel/          - Model output voxel temporal bins\n")
                f.write(f"\nData Format:\n")
                f.write(f"- Events: (N, 4) [t, x, y, p]\n")
                f.write(f"- Voxel: (8, 480, 640) [8 temporal bins]\n")
                f.write(f"- Duration: 20ms per segment\n")
                
            self.logger.info(f"ğŸ› Debug summary saved to {summary_file}")
            
        except Exception as e:
            self.logger.warning(f"ğŸ› Failed to generate debug summary: {e}")